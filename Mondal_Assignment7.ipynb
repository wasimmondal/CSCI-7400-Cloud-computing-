{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 1.</font>\n",
        "<font color=\"blue\">Setting up spark pyspark.</font>"
      ],
      "metadata": {
        "id": "uMyh8sTznT8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csoLjzfman6z",
        "outputId": "98ac9f7e-1826-480c-dceb-05599547ffdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"> Importing SparkSession class </font> <br>\n",
        "<font color=\"blue\"> SparkSession.builder create a builder object to configure SparkSession </font> <br>\n",
        "<font color=\"blue\">.appName(\"Readingtextfile\") set the name  of the Spark Application. It is useful for identifying the job in the spark UI. </font> <br>\n",
        "<font color=\"blue\"> .getOrCreate If one spark session exist, use that , if not create a new one</font> <br>"
      ],
      "metadata": {
        "id": "FG_OGS8Epti9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Readingtextfile\").getOrCreate()"
      ],
      "metadata": {
        "id": "wfCDp6GJawNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 2: Data collection</font>\n",
        "<font color=\"blue\"> Uploading the text data file. Colab does not save. So, we have to upload every time. We can keep the file saved in Google drive </font> <br>"
      ],
      "metadata": {
        "id": "ESAqOfzPpyzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "rdd_data =files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "0p1MQe6Ha2WK",
        "outputId": "ff1cc025-d0ad-4364-9573-491f4039c2b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-163925e9-3ac9-465f-ba98-b12ec9487833\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-163925e9-3ac9-465f-ba98-b12ec9487833\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving amazon_cells_labelled.txt to amazon_cells_labelled.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 3.</font>\n",
        "<font color=\"blue\">Reading data file: Using spark.read.text() method, we are reading data from the uploaded file and store it as rdd</font>"
      ],
      "metadata": {
        "id": "WwXlROwFp3ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd = spark.read.text(\"/content/amazon_cells_labelled.txt\")"
      ],
      "metadata": {
        "id": "AWniWhDYa9LK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">Note that we have read the file as rdd not as dataframe. So, it is collection of strings where each strings represents a line from the input file. We have displayed this output below:</font>"
      ],
      "metadata": {
        "id": "dBN4ZAbSqB6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rdd.take(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kdUioppbBg7",
        "outputId": "b2994955-993f-4e5a-ec70-73d4f2f7f349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='So there is no way for me to plug it in here in the US unless I go by a converter.\\t0'),\n",
              " Row(value='Good case, Excellent value.\\t1'),\n",
              " Row(value='Great for the jawbone.\\t1'),\n",
              " Row(value='Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\\t0'),\n",
              " Row(value='The mic is great.\\t1'),\n",
              " Row(value='I have to jiggle the plug to get it to line up right to get decent volume.\\t0'),\n",
              " Row(value='If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\\t0'),\n",
              " Row(value='If you are Razr owner...you must have this!\\t1'),\n",
              " Row(value='Needless to say, I wasted my money.\\t0'),\n",
              " Row(value='What a waste of money and time!.\\t0')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split,col"
      ],
      "metadata": {
        "id": "iehFPEl0bGAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\"> Taking an RDD with column named \"value\". It generally common in spark. Then using split() method to break the string into an array of substring using \\t tab as delimiter. Then namming the resultant \"split_col()\" using .alias(). Next, we are taking first element from this split_col() and putting it in new column named as text. Similarly, we have created anotther column named as label and putting teh second element of split_col() in it. </font>"
      ],
      "metadata": {
        "id": "dFraSCHZqEgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create the parsed_df with both text and label\n",
        "parsed_df = rdd.withColumn(\"text\", split(col(\"value\"), \"\\t\").getItem(0)) \\\n",
        "               .withColumn(\"label\", split(col(\"value\"), \"\\t\").getItem(1).cast(\"integer\")) \\\n",
        "               .drop(\"value\")"
      ],
      "metadata": {
        "id": "9PlvaD0ybJe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 3.</font>\n",
        "<font color=\"blue\">Processing data before tokenization</font>"
      ],
      "metadata": {
        "id": "gWZpRKo2qSAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, col, lower"
      ],
      "metadata": {
        "id": "Sg7RsY4ZbYIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">Pyspark provides regexp_replace function to replace substring within a string column that macthes regular expression pattern. with a-zA-Z, we are keeping only letter from small a-z and capital A-Z. Thus, we processed the data by removing special characters, numbers, and other noise.</font>"
      ],
      "metadata": {
        "id": "s3QGf308qXzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_df = parsed_df.withColumn(\"clean_text\",\n",
        "    regexp_replace(  # Remove special chars, numbers, extra spaces\n",
        "        lower(       # Convert to lowercase\n",
        "            col(\"text\")\n",
        "        ),\n",
        "        \"[^a-zA-Z\\\\s]\", \"\"  # Keep only letters, whitespace\n",
        "    )\n",
        ").withColumn(\n",
        "    \"clean_text\",\n",
        "    regexp_replace(col(\"clean_text\"), \"\\\\s+\", \" \")  # Replace multiple spaces with one\n",
        ")"
      ],
      "metadata": {
        "id": "Ko5AZuPfbObr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import Tokenizer"
      ],
      "metadata": {
        "id": "6ZdUo47FbhfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 4.</font>\n",
        "<font color=\"blue\"> Tokenize the data.</font>"
      ],
      "metadata": {
        "id": "z5Bkkg2xqkQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(inputCol=\"clean_text\", outputCol=\"words\")\n",
        "tokenized_df = tokenizer.transform(cleaned_df)"
      ],
      "metadata": {
        "id": "IM3YFQpzbcxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 5.</font>\n",
        "<font color=\"blue\"> Removing stopwords.</font>"
      ],
      "metadata": {
        "id": "mYkz7Kiaq1Xv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StopWordsRemover"
      ],
      "metadata": {
        "id": "_qnVhM1ebnDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=stopwords)\n",
        "filtered_df = remover.transform(tokenized_df)"
      ],
      "metadata": {
        "id": "r_fp5LM_bqn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 6.</font>\n",
        "<font color=\"blue\"> Applying word2vec to convert tokens to numerical feature\n",
        "vectors</font>"
      ],
      "metadata": {
        "id": "Qj-NdphTrGLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler, Word2Vec"
      ],
      "metadata": {
        "id": "07Z816wCbu3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(vectorSize=100,minCount=5,inputCol=\"filtered_words\",outputCol=\"word2vec_features\")"
      ],
      "metadata": {
        "id": "uW-M63wdb1jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v_model = word2vec.fit(filtered_df)"
      ],
      "metadata": {
        "id": "YVdJrQ49rcIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = w2v_model.transform(filtered_df)"
      ],
      "metadata": {
        "id": "sLyEae3DreK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 7.</font>\n",
        "<font color=\"blue\"> Preparing the feature vector</font>"
      ],
      "metadata": {
        "id": "vWymFhe5ruQK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "assembler = VectorAssembler(inputCols=[\"word2vec_features\"],outputCol=\"features\")"
      ],
      "metadata": {
        "id": "hgyAgPq2b7ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = assembler.transform(result_df)"
      ],
      "metadata": {
        "id": "wI1g3nBWsD5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 8.</font>\n",
        "<font color=\"blue\">  Choose a classification algorithm - Random Forest Classifier</font>"
      ],
      "metadata": {
        "id": "4thcAnGysoU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier"
      ],
      "metadata": {
        "id": "m4JPJQ-NcGMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=5,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "BvY3rvqMcCfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 9.</font>\n",
        "<font color=\"blue\">  Creating the pipeline</font>"
      ],
      "metadata": {
        "id": "TdPTwz2ws7Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "gvbvYqzzcR4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[rf])"
      ],
      "metadata": {
        "id": "rvdGUGMicKpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 10.</font>\n",
        "<font color=\"blue\">  Split the data into training and test sets</font>"
      ],
      "metadata": {
        "id": "IeD99a8xtIZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data = result_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "LoGGlPKtcVve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 10.</font>\n",
        "<font color=\"blue\">  Train the model</font>"
      ],
      "metadata": {
        "id": "g_riBvuJtbUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "LCtU5AyDca4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 11.</font>\n",
        "<font color=\"blue\">  Make prediction</font>"
      ],
      "metadata": {
        "id": "T1zJ3xa0tlJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "68nsAq64cksg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 12.</font>\n",
        "<font color=\"blue\">  Evaluate the model</font>"
      ],
      "metadata": {
        "id": "KMY_Op3Stw0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "0XL4ZVdqcpJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 12.1.</font>\n",
        "<font color=\"blue\">  Evaluate the model: Binary classification evaluator (uses AUC by default) </font>"
      ],
      "metadata": {
        "id": "ZgEk5vWVuD8h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")"
      ],
      "metadata": {
        "id": "HBioYMTjctL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 12.2.</font>\n",
        "<font color=\"blue\">  Evaluate the model: Multiclass evaluator for accuracy, precision, recall, etc. </font>"
      ],
      "metadata": {
        "id": "BrKBRnutuaLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\",\n",
        "    predictionCol=\"prediction\"\n",
        ")"
      ],
      "metadata": {
        "id": "En9Te5K6cwo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 12.3.</font>\n",
        "<font color=\"blue\">  Evaluate the model: Calculate evaluation metrics  </font>"
      ],
      "metadata": {
        "id": "ZQ0BB1Dpul6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})"
      ],
      "metadata": {
        "id": "An__oJqEc0bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 12.4.</font>\n",
        "<font color=\"blue\">  Printing out the accuracy, precession, recall and f1 values  </font>"
      ],
      "metadata": {
        "id": "78HJ3Hmtu3iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjKuJ3XTc7zR",
        "outputId": "e3bb19b3-63cb-473c-9445-8ce294de49f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7201\n",
            "Accuracy: 0.6605\n",
            "Precision: 0.6627\n",
            "Recall: 0.6605\n",
            "F1 Score: 0.6573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.</font>\n",
        "<font color=\"blue\"> Investigation on how various factors affect/help with the model performance. What could be done to improve the model’s performance.  </font>"
      ],
      "metadata": {
        "id": "85jRm29vvLcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "metadata": {
        "id": "xGKVp3MVd03M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.1.</font>\n",
        "<font color=\"blue\">Cross-validation for hyperparameter tuning  </font>"
      ],
      "metadata": {
        "id": "ObYX8qP2y6hH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [50, 100]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()"
      ],
      "metadata": {
        "id": "lV8USRqXdtiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "1URm15sAd8Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = crossval.fit(train_data)"
      ],
      "metadata": {
        "id": "90nnyHRPeC8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cv_model.bestModel"
      ],
      "metadata": {
        "id": "cPqfqmyQmHwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_prediction = best_model.transform(test_data)"
      ],
      "metadata": {
        "id": "R3ZEqz7JmTRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_auc = binary_evaluator.evaluate(cv_prediction)"
      ],
      "metadata": {
        "id": "eUaWRvatmxsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6tkhXEhm_Sk",
        "outputId": "1af45662-2200-439a-e56c-9a2847c98d98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.2.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of tree grid  </font>"
      ],
      "metadata": {
        "id": "X9V9oOuuzZD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [100, 150]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()"
      ],
      "metadata": {
        "id": "gNhEsNslzi8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "Z10LVpAG0Py6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_model = crossval.fit(train_data)"
      ],
      "metadata": {
        "id": "AGojPXgd0Suz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model = cv_model.bestModel"
      ],
      "metadata": {
        "id": "GuFeeOkT0ijb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYEDEAKS0j5r",
        "outputId": "c203954f-94bb-4d77-9702-62adfc308a15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.6917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zpi_omxu0xCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.3.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of tree grid 150-200 </font>"
      ],
      "metadata": {
        "id": "k2d0N6hE0x0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [150, 200]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kIS7m3t05aq",
        "outputId": "fa51405c-ec3e-4c15-a423-c43bee8957d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.7111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.4.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of tree grid 200-250 </font>"
      ],
      "metadata": {
        "id": "tQvUjfig1veW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [200, 250]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOR37jWS10Xm",
        "outputId": "9acd2d9c-352b-424d-b1c1-b8ae50198b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.6926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.5.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of tree grid 250-300 </font>"
      ],
      "metadata": {
        "id": "OGAigmAd2Szw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [250, 300]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEutAOxI2Y_B",
        "outputId": "74035a62-e16a-4fca-a8d4-8f957b2238b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.7019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.6.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of tree grid 300-400 </font>"
      ],
      "metadata": {
        "id": "S75252Qg23Oy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [300, 400]) \\\n",
        "    .addGrid(rf.maxDepth, [3, 5]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lKaAcUx26bS",
        "outputId": "077d6825-3a9a-475e-f91c-5ea3fce82560"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.6998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.7.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of max depthe 10-20 </font>"
      ],
      "metadata": {
        "id": "1CegoJnK3eZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [50, 100]) \\\n",
        "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qw4iPQJi3ryT",
        "outputId": "318cdcc0-fbdc-4878-e313-3259d5fb2bf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.7347\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.8.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of max depthe 20-30 </font>"
      ],
      "metadata": {
        "id": "t9PSzcXt4LEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [50, 100]) \\\n",
        "    .addGrid(rf.maxDepth, [20, 30]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDMZvHzW4Pn9",
        "outputId": "756bffd9-e7b3-492e-cdd6-ddf771511394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best model AUC on test data = 0.6989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.8.</font>\n",
        "<font color=\"blue\">Hyperparameter tuning: Effect of number of max depthe 30-40 </font>"
      ],
      "metadata": {
        "id": "jkx19xdo4_Se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "paramGrid = ParamGridBuilder() \\\n",
        "    .addGrid(rf.numTrees, [50, 100]) \\\n",
        "    .addGrid(rf.maxDepth, [30, 40]) \\\n",
        "    .addGrid(rf.impurity, [\"gini\", \"entropy\"]) \\\n",
        "    .build()\n",
        "crossval = CrossValidator(\n",
        "    estimator=pipeline,\n",
        "    estimatorParamMaps=paramGrid,\n",
        "    evaluator=binary_evaluator,  # Optimizing for AUC\n",
        "    numFolds=3,\n",
        "    seed=42\n",
        ")\n",
        "cv_model = crossval.fit(train_data)\n",
        "best_model = cv_model.bestModel\n",
        "cv_prediction = best_model.transform(test_data)\n",
        "cv_auc = binary_evaluator.evaluate(cv_prediction)\n",
        "print(f\"Best model AUC on test data = {cv_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "KivPA2vj5EIQ",
        "outputId": "534e9c5c-7a6e-4435-96a1-b4a6dfcdf5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "RandomForestClassifier_7799b4b5aee6 parameter maxDepth given invalid value 40.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-3a7bdcad6622>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcv_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mcv_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             raise TypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0m__next__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m                    \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mwrap_exception\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_helper_reraises_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0m_parallelFitTasks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meva\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             )\n\u001b[0;32m--> 847\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mmetrics_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/util.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetLocalProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36msingleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msingleTask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelIter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;31m# TODO: duplicate evaluator to take extra params from input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m#  Note: Supporting tuning params in evaluator need update method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No models remaining.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfitSingleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamMaps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_FitMultipleIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitSingleModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparamMaps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, extra)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mthat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mextra\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mthat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mstages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetStages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, extra)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_empty_java_param_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m             \u001b[0mthat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_transfer_params_to_java\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0mpair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_java_param_pair\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_paramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_make_java_param_pair\u001b[0;34m(self, param, value)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mjava_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetParam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mjava_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_py2java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mjava_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: RandomForestClassifier_7799b4b5aee6 parameter maxDepth given invalid value 40."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "laWzjGld7bkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.9.</font>\n",
        "<font color=\"blue\">Changing parameter in logistic regression </font>"
      ],
      "metadata": {
        "id": "NBxZ1d8x7gLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=10,\n",
        "    seed=42\n",
        ")"
      ],
      "metadata": {
        "id": "gq2BEPP27rkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[rf])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxxLuznV79UP",
        "outputId": "d765a7e9-ec4c-465a-af16-5dfa2176f3b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7347\n",
            "Accuracy: 0.6728\n",
            "Precision: 0.6754\n",
            "Recall: 0.6728\n",
            "F1 Score: 0.6697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.9.</font>\n",
        "<font color=\"blue\">Changing parameter in logistic regression </font>"
      ],
      "metadata": {
        "id": "IUwsAd3F8NBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=12,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMfr-oWS8QwQ",
        "outputId": "ff907e5e-bf78-47c9-b008-6ebabba7fbb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7304\n",
            "Accuracy: 0.6852\n",
            "Precision: 0.6896\n",
            "Recall: 0.6852\n",
            "F1 Score: 0.6813\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.9.</font>\n",
        "<font color=\"blue\">Changing parameter in logistic regression </font>"
      ],
      "metadata": {
        "id": "p47FXDGx8sxf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=50,\n",
        "    maxDepth=12,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrctdInM8wQ5",
        "outputId": "29712b2c-f8e1-4ba7-9f9d-378e3743cc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.6999\n",
            "Accuracy: 0.6543\n",
            "Precision: 0.6541\n",
            "Recall: 0.6543\n",
            "F1 Score: 0.6541\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=150,\n",
        "    maxDepth=12,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu33VV0Y83aQ",
        "outputId": "2217cd98-15ba-4dd2-9736-2758e8ae12ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7073\n",
            "Accuracy: 0.6605\n",
            "Precision: 0.6668\n",
            "Recall: 0.6605\n",
            "F1 Score: 0.6542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 13.9.</font>\n",
        "<font color=\"blue\">Changing parameter in logistic regression </font>"
      ],
      "metadata": {
        "id": "KT0W21q98bIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose a classification algorithm - Random Forest Classifier\n",
        "rf = RandomForestClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    numTrees=100,\n",
        "    maxDepth=14,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[rf])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdMoyscJ8fRo",
        "outputId": "3ab27739-fa10-4c41-af12-2f7ca9779074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7305\n",
            "Accuracy: 0.6728\n",
            "Precision: 0.6767\n",
            "Recall: 0.6728\n",
            "F1 Score: 0.6688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 14</font>\n",
        "<font color=\"blue\">Different model:Logistic regression classifier  </font>"
      ],
      "metadata": {
        "id": "2gQ3O2a05Pjw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression"
      ],
      "metadata": {
        "id": "kux-XPfi5l4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Logistic Regression classifier\n",
        "lr = LogisticRegression(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    family=\"binomial\",  # Explicitly set for binary classification\n",
        "    elasticNetParam=0.8,  # Balance between L1 (Lasso) and L2 (Ridge) regularization\n",
        "    regParam=0.1,       # Regularization strength\n",
        "    maxIter=100,\n",
        "    tol=1e-6\n",
        ")\n"
      ],
      "metadata": {
        "id": "U8AKT0Bi51ox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline(stages=[lr])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUE8_QhG53lC",
        "outputId": "1a4eab9b-49bc-4ba6-c3d9-b04ccca4f07f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.6322\n",
            "Accuracy: 0.5864\n",
            "Precision: 0.5862\n",
            "Recall: 0.5864\n",
            "F1 Score: 0.5825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"red\">Step 15</font>\n",
        "<font color=\"blue\">Different model:Gradient Boost classifier  </font>"
      ],
      "metadata": {
        "id": "dZ4IT_hY9GiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=100,\n",
        "    maxDepth=5,\n",
        "    stepSize=0.01,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[gbt])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F19YCT7n9ZWJ",
        "outputId": "0d061baa-83cf-4c31-8960-2c45df403a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.6717\n",
            "Accuracy: 0.6358\n",
            "Precision: 0.6355\n",
            "Recall: 0.6358\n",
            "F1 Score: 0.6350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import GBTClassifier\n",
        "gbt = GBTClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxIter=100,\n",
        "    maxDepth=10,\n",
        "    stepSize=0.01,\n",
        "    seed=42\n",
        ")\n",
        "pipeline = Pipeline(stages=[gbt])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNdb1xA0-G30",
        "outputId": "17f96e3a-1f01-4918-dca6-8c88619e2521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.6528\n",
            "Accuracy: 0.5864\n",
            "Precision: 0.5866\n",
            "Recall: 0.5864\n",
            "F1 Score: 0.5865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "svm = LinearSVC(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    regParam=0.1,\n",
        "    maxIter=100,\n",
        "    standardization=True  # Auto-scales features\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[svm])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORfiUD46E0-d",
        "outputId": "d88f392e-18ef-4314-b7c6-75492c3327e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7376\n",
            "Accuracy: 0.6728\n",
            "Precision: 0.6782\n",
            "Recall: 0.6728\n",
            "F1 Score: 0.6679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "svm = LinearSVC(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    regParam=0.2,\n",
        "    maxIter=100,\n",
        "    standardization=True  # Auto-scales features\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[svm])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t28MiDbcFNWu",
        "outputId": "c1a38eea-122b-4650-fe0b-a459bbafbeae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7198\n",
            "Accuracy: 0.6605\n",
            "Precision: 0.6668\n",
            "Recall: 0.6605\n",
            "F1 Score: 0.6542\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LinearSVC\n",
        "svm = LinearSVC(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    regParam=0.1,\n",
        "    maxIter=1000,\n",
        "    standardization=True  # Auto-scales features\n",
        ")\n",
        "\n",
        "\n",
        "pipeline = Pipeline(stages=[svm])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCr5MxOgFe0f",
        "outputId": "527b3883-0a7c-48d8-aafb-d117989005c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7387\n",
            "Accuracy: 0.6728\n",
            "Precision: 0.6782\n",
            "Recall: 0.6728\n",
            "F1 Score: 0.6679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier(\n",
        "    featuresCol=\"features\",\n",
        "    labelCol=\"label\",\n",
        "    maxDepth=5,\n",
        "    minInstancesPerNode=10,\n",
        "    impurity=\"gini\",\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "pipeline = Pipeline(stages=[svm])\n",
        "model = pipeline.fit(train_data)\n",
        "predictions = model.transform(test_data)\n",
        "auc = binary_evaluator.evaluate(predictions)\n",
        "accuracy = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"accuracy\"})\n",
        "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
        "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
        "f1 = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
        "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTgPYmolFziP",
        "outputId": "c7bea89f-bc3a-41fc-dbbc-6eb12639ded2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Area Under ROC (AUC): 0.7387\n",
            "Accuracy: 0.6728\n",
            "Precision: 0.6782\n",
            "Recall: 0.6728\n",
            "F1 Score: 0.6679\n"
          ]
        }
      ]
    }
  ]
}